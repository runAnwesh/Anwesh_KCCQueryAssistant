**An offline-capable, local-first AI application** that allows users to query agricultural advice from the **Kisan Call Center (KCC) dataset**. If no relevant local context is found, it seamlessly falls back to live Internet search.  
Built as part of the AI Engineer assignment.

---

## ğŸš€ Features

âœ… Local LLM (DeepSeek via Ollama) for answering queries  
âœ… Retrieval-Augmented Generation (RAG) pipeline using FAISS  
âœ… Offline-capable vector search  
âœ… Streamlit web app for user-friendly querying  
âœ… Live fallback search when no local data is relevant  

---

## ğŸ“¦ Repository Structure

```

.
â”œâ”€â”€ app/                  # Streamlit app
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ data/                 # Raw & processed data
â”‚   â”œâ”€â”€ raw/              # Raw KCC dataset
â”‚   â”œâ”€â”€ processed/        # Cleaned JSON data
|   â””â”€â”€data_ingestion.ipynb     # Data collection notebook
â”œâ”€â”€ llm/                  # Ollama LLM integration
â”‚   â””â”€â”€ launch\_llm.sh
â”œâ”€â”€ scripts/              # Core pipeline scripts
â”‚   â”œâ”€â”€ preprocess\_data.py
â”‚   â”œâ”€â”€ generate\_embeddings.py
â”‚   â”œâ”€â”€ semantic\_search.py
â”‚   â”œâ”€â”€ ollama\_client.py
â”‚   â””â”€â”€ rag\_pipeline.py
â”œâ”€â”€ vector\_store/         # FAISS index & document store
â”œâ”€â”€ assets/               # Sample queries & assets
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # Project overview & instructions

````

---

## ğŸ“ Workflow Overview

### 1ï¸âƒ£ Data Ingestion
- **Source**: Public KCC dataset (`data/raw/KCC_raw_data.csv`)
- **Script**: `scripts/data_ingestion.ipynb` and `scripts/preprocess_data.py`
- **Output**: Cleaned & normalized JSON dataset in `data/processed/KCC_processed_data.json`

### 2ï¸âƒ£ Local LLM Deployment
- **Model**: `deepseek` via Ollama
- **Script**: `llm/launch_llm.sh`  
- **Note**: Runs completely offline using quantized models if needed.

### 3ï¸âƒ£ Embedding Generation & Vector Store
- **Embedding Model**: `sentence-transformers/paraphrase-MiniLM-L6-v2`
- **Script**: `scripts/generate_embeddings.py`
- **Output**: FAISS index and document store in `vector_store/`

### 4ï¸âƒ£ Retrieval-Augmented Generation (RAG) Pipeline
- **Script**: `scripts/rag_pipeline.py`
- **Function**:
  - Semantic search for top-k context chunks
  - Local LLM response if relevant context found
  - Fallback live search (using Tavily Search API) if relevance threshold not met

### 5ï¸âƒ£ User Interface (Streamlit App)
- **Script**: `app/main.py`
- **Features**:
  - Natural language queries
  - Displays structured LLM answers (highlighted)
  - Fallback answers clearly indicated

---

## ğŸ”¬ Sample Queries
- â€œWhat pest-control methods are recommended for paddy in Tamil Nadu?â€
- â€œHow to manage drought stress in groundnut cultivation?â€
- â€œWhat issues do sugarcane farmers in Maharashtra commonly face?â€
- (and at least 10 additional queries included in `assets/sample_queries.txt`)

---

## âš™ï¸ Installation & Running

1ï¸âƒ£ Clone the repo:
```bash
git clone https://github.com/runAnwesh/Anwesh_KCCQueryAssistant.git
cd Anwesh_KCCQueryAssistant
````

2ï¸âƒ£ Install dependencies:

```bash
pip install -r requirements.txt
```

3ï¸âƒ£ Preprocess data:

```bash
python scripts/preprocess_data.py
```

4ï¸âƒ£ Generate embeddings:

```bash
python scripts/generate_embeddings.py
```

5ï¸âƒ£ Launch Ollama LLM:

```bash
cd llm
./launch_llm.sh
```

6ï¸âƒ£ Run Streamlit app:

```bash
cd ..
streamlit run app/main.py
```

---

## ğŸ¥ Demo Video

Check out the 3â€“5 min screencast covering:

* Local startup of LLM and vector store
* 3-5 queries returning KCC-based answers
* 2-3 queries falling back to live Internet search

**\[Google Drive Demo Video Link Here]**

---

## ğŸ“š Technical Documentation

All scripts are heavily documented for clarity:

* Data ingestion & preprocessing (`scripts/preprocess_data.py`)
* Embedding generation (`scripts/generate_embeddings.py`)
* Vector store ingestion (`scripts/generate_embeddings.py`)
* RAG pipeline & LLM integration (`scripts/rag_pipeline.py`, `ollama_client.py`)
* Streamlit app (`app/main.py`)

---

## ğŸ Final Notes

âœ… This project is built fully local-first with fallback search to ensure reliability
âœ… Modular scripts allow easy swapping of embedding models or fallback search APIs
âœ… Ready to deploy and extend for future enhancements (e.g., Bing API fallback, fine-tuned LLMs)

---

Feel free to reach out if youâ€™d like help customizing this further, deploying on cloud, or extending features. Enjoy your fully functional KCC Query Assistant! ğŸš€ğŸŒ¾

```

---


